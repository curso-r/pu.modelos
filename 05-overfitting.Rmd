---
title: "05-overfitting"
---

## Overfitting

*Overfitting* ou *superajuste* acontece quando a função $f$ estimada por algum 
modelo da forma $y = f(x) + \epsilon$ é muito específica sendo assim, quando avaliamos
o modelo em um outro conjunto de observações percebemos que o erro aumenta muito.

Isso acontece quando o modelo aprende muitos detalhes e ruidos da base de treino e 
ao aplicar o modelo em novos dados, como esses detalhes/ruídos não se repetem, a
performance do modelo é impactada de forma negativa.

Para visualizar o que é overfitting, considere o seguinte banco de dados.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
set.seed(7)
dados <- data_frame(
  x = runif(10),
  y = 2*x + rnorm(10, 0, 0.1)
)
ggplot(dados, aes(x = x, y = y)) + geom_point()
```

Esse banco de dados foi gerado usando exatamente as suposições de um modelo de regressão.
Temos uma variável $x$ e uma variável $y$ que é calculada com $2*x + \epsilon$ em que
$\epsilon$ é uma variável aleatória com distribuição Normal de média zero e desvio padrão $0.1$.

Portanto, o melhor modelo para explicar esses dados, seria um modelo linear bem simples,
que poderia ser ajsutado no R usando:

```{r}
modelo <- lm(y ~ x, data = dados)
summary(modelo)
```

Note que mesmo com 10 observações o modelo acertou precisamente os parâmetros que 
utilizamos para simular os dados. Mas existe uma aleatoriedade inerente ao método
que utilizamos para construir o banco de dados.

Imagine se, ao invés de ajustar esse modelo, tivessemos ajustado o modelo:

$$y = \alpha + \beta_1x + \beta_2x^2 + ... + \beta_9x^9 + \epsilon$$
No R:

```{r}
modelo2 <- lm(y ~ poly(x, 9), data = dados)
summary(modelo2)
```

Veja agora o gráfico dos modelos ajsutados:

```{r}
ggplot(dados, aes(x = x, y = y)) + geom_point() + 
  geom_smooth(formula = y ~ x, colour = "red", se = FALSE, method = 'lm') +
  geom_smooth(formula = y ~ poly(x, 9), se = FALSE, method = 'lm')
```

O linha em vermelho, é a reta ajustada pelo primeiro modelo, ou seja, o modelo que 
utilizamos para gerar os dados. A linha azul, é a curva ajustada pelo polinômio do
nono grau. O modelo azul acerta todos os pontos enquanto o vermelho (que é o modelo
correto não). Se calcularmos o erro médio quadrático de cada um dos modelos, chegaríamos
a conclusão de que o modelo azul é melhor.

```{r}
erro_modelo1 <- mean((dados$y - predict(modelo, newdata = dados))^2)
erro_modelo2 <- mean((dados$y - predict(modelo2, newdata = dados))^2)
erro_modelo1 %>% round(3)
erro_modelo2 %>% round(3)
```

Mas e gerarmos mais dados de acordo com o nosso modelo inicial? Qual modelo terá melhor
performance? 

```{r}
dados2 <- data_frame(
  x = runif(100),
  y = 2*x + rnorm(100, 0, 0.1)
)
ggplot(dados2, aes(x = x, y = y)) + geom_point() +
  geom_smooth(data = dados, formula = y ~ x, colour = "red", se = FALSE, method = 'lm') +
  geom_smooth(data = dados, formula = y ~ poly(x, 9), se = FALSE, method = 'lm')
erro_modelo1 <- mean((dados2$y - predict(modelo, newdata = dados2))^2)
erro_modelo2 <- mean((dados2$y - predict(modelo2, newdata = dados2))^2)
erro_modelo1 %>% round(3)
erro_modelo2 %>% round(3)
```

O modelo que acertava todas as observações na base que usamos para treinar, passou
a errar mais quando testado em novos dados.

Isso é o que chamamos de *overfitting*. O modelo azul ajustou ruidos aleatórios que
eram inerentes à forma com que os dados foram gerados e dessa forma, não foi capaz
de prever bem em dados que tinham ruidos aleatorios diferentes.

Claro, esse exemplo é ilustrativo. Desde o começo sabíamos a forma com que os dados
eram gerados. Isso raramente acontece. Na prática, estamos tentando criar um modelo 
para explicar como os dados são gerados, por isso temos que tomar bastante cuidado 
para não assumir relações desta forma e criar modelos que explicam apenas aquela amostra.

