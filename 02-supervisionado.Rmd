---
title: "Aprendizado Supervisionado"
---

```{r, message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, 
  out.width = "50%", out.height = "50%",
  fig.retina = 2
)
```

## Aprendizado Supervisionado

Suponha que você observou uma variável resposta $Y$ e $p$ diferentes variáveis 
explicativas $X_1, X_2, ..., X_p$. Assumimos que existe alguma relação entre $Y$
e $X = (X_1, X_2, ..., X_p)$. Podemos denotar matematicamente esta relação como
na seguinte equação:

$$Y = f(X) + \epsilon$$

O objetivo geral do aprendizado supervisionado é estimar a função $f$.
Nessa formulação, $\epsilon$ é um termo de erro aleatório com média 0. $f$ representa
a informação sistemática que $X$ fornece sobre $Y$.

Existem diversas maneiras de estimar essa função. Em alguns casos assumimos uma
forma paramétrica para ela, em outros não. Alguns exemplos de algoritmos são:

* Regressão Linear
* Regressão Logística
* Árvore de Decisão
* Florestas Aleatórias (*Random Forest*)
* Gradient Boosting
* Redes Neurais
* Etc.

Cada um dos algoritmos possui as suas vantagens e desvantagens, e problemas em 
que trazem melhores resultados ou não. 


### Árvore  de Decisão

Os modelos de árvore de decisão como vamos utilizar são implementados de acordo
com o livro *Classification and Regression Trees* de Breiman, Friedman, Olshen e Stone.
No R, o pacote que usamos para fazer este tipo de análise é o `rpart`. Uma 
curiosidade é que gostariam que os autores do pacote gostariam de usar o nome `cart`,
mas esse nome foi utilizado por uma implementação particular dessas ideias. No fim,
ficou mais famoso o `rpart`, mostrando a importância do software livre.

Não vamos entrar matematicamente no detalhe de como funciona uma árvore de decisão.
Para entender como funciona um árvore de decisão, imagine que você tem um nó com
$N$ observações e que $n$ possuem $Resposta = 1$ e $N - n$ possuem $Resposta = 0$, 
ou seja, temos um problema de classificação binária. Então neste caso $p = \frac{n}{N}$
é a proporção de resposta neste nó.

O objetivo da árvore de decisão dividir este nó em 2 de forma que a diferença entre
a proporção de respostas entre os dois nós resultantes seja a maior possível. Claro que 
cada um dos nós precisa ter uma quantidade significativa de observações de forma que $p$ 
seja estimado corretamente.

Uma introdução mais formal a esses métodos pode ser encontrada na vignette do pacote 
`rpart`. Digite `vignette('longintro', package = 'rpart')` no console para encontrá-la.

### Exemplo 

Para esse exemplo vamos usar o banco de dados do Titanic. Um banco de dados que
ficou famoso por causa de uma competição no Kaggle. Esse banco de dados contém 
diversas informações sobre os passageiros do Titanic bem como uma variável que 
indica se o passageiro sobreviveu (1) e se não sobreviveu (0).

```{r, warning=FALSE}
library(readr)
titanic <- read_csv('data/titanic-train.csv')
```

















